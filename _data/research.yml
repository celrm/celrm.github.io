list:
- title: "HAM: A hyperbolic step to regulate implicit bias"
  author_array: [Tom Jacobs, Advait Gadhikar, Celia Rubio-Madrigal, Rebekka Burkholz]
  type: article
  journal: Pre-print on arXiv
  year: 2025
  url: https://arxiv.org/abs/2506.02630
  arxiv: "2506.02630"
  abstract: "Understanding the implicit bias of optimization algorithms has become central to explaining the generalization behavior of deep learning models. For instance, the hyperbolic implicit bias induced by the overparameterization m ⊙ w—though effective in promoting sparsity—can result in a small effective learning rate, which slows down convergence. To overcome this obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates between an optimizer step and a new hyperbolic mirror step. We derive the Riemannian gradient flow for its combination with gradient descent, leading to improved convergence and a similar beneficial hyperbolic geometry as m ⊙ w for feature learning. We provide an interpretation of the the algorithm by relating it to natural gradient descent, and an exact characterization of its implicit bias for underdetermined linear regression. HAM's implicit bias consistently boosts performance —even of dense training, as we demonstrate in experiments across diverse tasks, including vision, graph and node classification, and large language model fine-tuning. HAM is especially effective in combination with different sparsification methods, improving upon the state of the art. The hyperbolic step requires minimal computational and memory overhead, it succeeds even with small batch sizes, and its implementation integrates smoothly with existing optimizers."
  img: ham-hyperbolic-step.png
  categories: [cs.LG]
  preprint: true

- title: "When shift happens - Confounding is to blame"
  author_array: [Abbavaram Gowtham Reddy, Celia Rubio-Madrigal, Rebekka Burkholz, Krikamol Muandet]
  type: article
  journal: Pre-print on arXiv
  year: 2025
  url: https://arxiv.org/abs/2505.21422
  arxiv: 2505.21422
  abstract: "Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its OOD generalization performance improves when all available covariates, not just causal ones, are utilized. Drawing on both empirical and theoretical evidence, we attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing OOD generalization approaches. Under such conditions, we prove that effective generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we show that models augmented with proxies for hidden confounders can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance for designing robust OOD generalization algorithms and principled covariate selection strategies."
  img: when-shift-happens.png
  categories: [cs.LG]
  preprint: true

- title: "GNNs getting ComFy: Community and feature similarity guided rewiring"
  author_array: [Celia Rubio-Madrigal*, Adarsh Jamadandi*, Rebekka Burkholz]
  type: article
  journal: Thirteenth International Conference on Learning Representations (ICLR)
  year: 2025
  url: https://openreview.net/forum?id=g6v09VxgFw
  arxiv: 2502.04891
  abstract: "Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights."
  code: https://github.com/relationalml/comfy
  img: gnns-getting-comfy.png
  poster: ComFy__poster_ICLR25.pdf
  categories: [cs.LG, cs.SI]
  first_author: true

- title: Spectral graph pruning against over-squashing and over-smoothing
  author_array: [Adarsh Jamadandi*, Celia Rubio-Madrigal*, Rebekka Burkholz]
  type: article
  journal: Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS)
  year: 2024
  url: https://openreview.net/forum?id=EMkrwJY2de
  arxiv: 2404.04612
  abstract: "Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets."
  code: https://github.com/relationalml/spectralpruningbraess
  img: spectral-pruning.png
  poster: Spectral_Pruning__poster_NeurIPS24.pdf
  categories: [cs.LG, cs.SI]
  first_author: true

- title: Rendering string diagrams recursively
  author_array: [Celia Rubio-Madrigal, Jules Hedges]
  type: article
  journal: Pre-print on arXiv
  year: 2024
  url: https://arxiv.org/pdf/2404.02679
  arxiv: 2404.02679
  abstract: "String diagrams are a graphical language used to represent processes that can be composed sequentially or in parallel, which correspond graphically to horizontal or vertical juxtaposition. In this paper we demonstrate how to compute the layout of a string diagram by folding over its algebraic representation in terms of sequential and parallel composition operators. The algebraic representation can be seen as a term of a free monoidal category or a proof tree for a small fragment of linear logic. This contrasts to existing non-compositional approaches that use graph layout techniques. The key innovation is storing the diagrams in binary space-partition trees, maintaining a right-trapezoidal shape for the diagram's outline as an invariant. We provide an implementation in Haskell, using an existing denotational graphics library called Diagrams. Our renderer also supports adding semantics to diagrams to serve as a compiler, with matrix algebra used as an example."
  code: https://github.com/celrm/stringdiagrams
  img: rendering-string-diagrams.png
  categories: [math.CT, cs.CG]
  preprint: true

- title: Analysis of a neural network for the identification of complex Boolean functions
  author_array: [Celia Rubio-Madrigal]
  advisors: Ismael Rodríguez Laguna, Daniel Loscos Barroso
  type: thesis
  school: Bachelor's thesis in Computer Science at Universidad Complutense de Madrid
  year: 2022
  url: https://hdl.handle.net/20.500.14352/3235
  abstract: "The goal of this work is to propose a possible long-term strategy to address the P vs. NP problem, based on studying class P/poly and Boolean circuit complexity. The main peculiarity of our strategy is that it will try to attain theoretical knowledge though empirical means. To do this we will use neural networks, and we will analyze both their structure and their performance. With the knowledge obtained we can either confirm, support or refute our theoretical hypotheses; or even formulate new ones.On the one hand, we will formalize notions related to the repetitiveness of Boolean functions, and we will define metrics associated with them under the assumption that simpler functions are more repetitive. On the other hand, we will try to find patterns in the weights of a neural network trained with the truth tables of some Boolean functions, which classifies them according to the size of the minimum circuit that computes them. Finally, by training more neural networks, we will analyze how the identified patterns and repeatability metrics behave as classifiers when put against truth tables."
  code: https://github.com/celrm/tfg
  img: analysis-of-a-nn.png
  categories: [cs.CC, cs.LG]
  preprint: true

categories:
- cs.LG: Machine Learning
- cs.SI: Social and Information Networks
- cs.CG: Computational Geometry
- math.CT: Category Theory
- cs.CC: Computational Complexity